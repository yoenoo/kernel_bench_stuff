from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "cognition-ai/Kevin-32B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
  model_name,
  torch_dtype=torch.bfloat16,
  device_map="auto"
)

prompt = "What is the capital of France?"
prompt = """
You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \n\n    You have complete freedom to choose the set of operators you want to replace. You may make the decision to replace some operators with custom CUDA kernels and leave others unchanged. You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n\n\n        Here\'s an example to show you the syntax of inline embedding custom CUDA operators in torch: The example given architecture is: \n\n        ``` \n\n        import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, a, b):\n        return a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n        ``` \n\n        The example new arch with custom CUDA kernels looks like this: \n        ```\n        import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nelementwise_add_source = """\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n\n    return out;\n}\n"""\n\nelementwise_add_cpp_source = (\n    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);"\n)\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name="elementwise_add",\n    cpp_sources=elementwise_add_cpp_source,\n    cuda_sources=elementwise_add_source,\n    functions=["elementwise_add_cuda"],\n    verbose=True,\n    extra_cflags=[""],\n    extra_ldflags=[""],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elementwise_add = elementwise_add\n\n    def forward(self, a, b):\n        return self.elementwise_add.elementwise_add_cuda(a, b)\n\n        ``` \n\n        \n    You are given the following architecture: \n\n    ```\n    import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    """\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    """\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        """\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        """\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n    ```\n    \nOptimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \n\nThe goal is to get the fastest inference speed as much as possible. Please think step by step. \n\nFirst, note that the default `torch.matmul` may not be optimal here because the K dimension is extremely large (e.g., 131072 * 4 = 524,288), and the matrix multiplication may have high arithmetic intensity. To maximize the GPU\'s floating-point throughput, we should design a custom kernel that efficiently utilizes the GPU\'s memory hierarchy and thread blocking. \n\nOne approach is to use a blocked algorithm, where we divide the matrices into blocks that fit into shared memory, allowing threads to reuse data multiple times. This reduces global memory access and improves performance. \n\nThe standard matrix multiplication kernel can be optimized using a tiling approach. Each block of threads is responsible for a tile of the output matrix, and each thread computes a few elements within that tile. The matrices A and B are loaded into shared memory blocks in tiles, so that multiple elements of the output can be computed with the same data from A and B.\n\nThe kernel will have the following structure:\n\n1. Each block is responsible for a tile of the output matrix C of size (BLOCK_SIZE x BLOCK_SIZE).\n2. Each thread in the block computes one element of this tile.\n3. The matrices A and B are divided into blocks of size (BLOCK_SIZE x K) and (K x BLOCK_SIZE), respectively.\n4. The kernel uses shared memory to store the tiles of A and B for the block.\n5. The computation is performed in multiple iterations, each loading a tile of A and B into shared memory, then performing the dot product for the respective tile.\n\nChoosing a suitable block size is critical. A common choice is 16x16 or 32x32. Given the large K dimension, a larger block size might be better to maximize the reuse. Let\'s choose 16x16 for simplicity, but this can be tuned.\n\nNow, implementing this in CUDA:\n\nThe kernel will have the following steps:\n\n- Each block is assigned a tile in the output matrix C.\n- Each thread in the block computes one element in the tile.\n- The kernel uses shared memory arrays for the tiles of A and B.\n- The kernel loops over the K dimension in chunks of the tile size, accumulating the result.\n\nThe shared memory size needs to be BLOCK_SIZE * BLOCK_SIZE * 2 (for A and B tiles). For BLOCK_SIZE=16, this is 16*16*2 = 512 bytes per block, which is manageable.\n\nNow, writing the kernel:\n\nThe kernel function will take pointers to A, B, and C, along with the dimensions M, N, K. Since in the problem, M and N are 256, and K is 524,288.\n\nThe kernel code:\n\n```cuda\n__global__ void matrix_multiply_kernel(const float* A, const float* B, float* C, \n                                      int M, int N, int K) {\n    __shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float shared_B[BLOCK_SIZE][BLOCK_SIZE];\n\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n\n    float C_value = 0.0;\n\n    for (int k = 0; k < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; ++k) {\n        // Load the current block of A and B into shared memory\n        int a_row = row;\n        int a_col = k * BLOCK_SIZE + threadIdx.x;\n        if (a_col < K) {\n            shared_A[threadIdx.y][threadIdx.x] = A[a_row * K + a_col];\n        } else {\n            shared_A[threadIdx.y][threadIdx.x] = 0.0;\n        }\n\n        int b_row = k * BLOCK_SIZE + threadIdx.y;\n        int b_col = col;\n        if (b_row < K) {\n            shared_B[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            shared_B[threadIdx.y][threadIdx.x] = 0.0;\n        }\n\n        __syncthreads();\n\n        // Compute the product of the shared matrices and accumulate\n        for (int i = 0; i < BLOCK_SIZE; ++i) {\n            C_value += shared_A[threadIdx.y][i] * shared_B[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = C_value;\n    }\n}\n```\n\nWait, there might be some errors here. Let me check the indices again.\n\nActually, the thread indices need to be properly mapped. The block dimensions should be arranged such that each block is responsible for a BLOCK_SIZE x BLOCK_SIZE tile in the output matrix. The thread block should be of size BLOCK_SIZE x BLOCK_SIZE, and the grid dimensions should be (ceil(N/BLOCK_SIZE), ceil(M/BLOCK_SIZE)).\n\nIn the kernel, the row and column indices for the output matrix C are computed as:\n\nrow = blockIdx.y * BLOCK_SIZE + threadIdx.y\n\ncol = blockIdx.x * BLOCK_SIZE + threadIdx.x\n\nEach thread in the block is responsible for computing C[row][col].\n\nThe shared memory for A and B should be of size BLOCK_SIZE x BLOCK_SIZE. However, in the loading step, for matrix A, each thread in the block needs to load a element from A\'s row (row) and column (k*BLOCK_SIZE + thread_x). Wait, actually, the A matrix is of size MxK, so each element in A is A[row][a_col], where a_col ranges from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1. Similarly, B is KxN, so each element in B is B[b_row][col], where b_row is from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1.\n\nBut in the shared memory, the arrangement might need to be transposed for B to allow better coalescing. Alternatively, the B matrix can be loaded as B[b_row][col] but transposed in shared memory.\n\nWait, perhaps the shared_B should be loaded as B\'s columns in the block. Let me think again.\n\nThe standard approach is to tile the K dimension. For each tile k, load a BLOCK_SIZE x BLOCK_SIZE tile from A and a BLOCK_SIZE x BLOCK_SIZE tile from B, then compute the dot product.\n\nActually, the correct way is:\n\n- For matrix A: each block\'s tile in C is at (blockIdx.y, blockIdx.x). The corresponding rows in A are the same as the rows in C (row = blockIdx.y * BLOCK_SIZE + threadIdx.y), and the columns are the current tile k * BLOCK_SIZE to (k+1)*BLOCK_SIZE.\n\n- For matrix B: the rows correspond to the current tile k * BLOCK_SIZE to (k+1)*BLOCK_SIZE, and the columns are the same as the columns in C (col = blockIdx.x * BLOCK_SIZE + threadIdx.x).\n\nTherefore, the A tile for shared memory is:\n\nA\'s row is fixed (row), and the column is k * BLOCK_SIZE + threadIdx.x.\n\nWait, no. Each thread in the block needs to load a element from A and B.\n\nWait, the shared_A is a BLOCK_SIZE x BLOCK_SIZE matrix. Each thread in the block (threadIdx.y, threadIdx.x) will load A\'s element at (row, k * BLOCK_SIZE + threadIdx.x). So for shared_A, the thread\'s x index corresponds to the column in the current tile of A.\n\nSimilarly, for shared_B, the thread\'s y index corresponds to the row in the current tile of B. So B\'s element is at (k * BLOCK_SIZE + threadIdx.y, col).\n\nWait, actually, to make the matrix multiplication work, the shared_A should be of size BLOCK_SIZE (rows) x BLOCK_SIZE (columns), and shared_B should be BLOCK_SIZE (rows) x BLOCK_SIZE (columns). Then, the product of shared_A and shared_B would contribute to the current block\'s tile in C.\n\nBut actually, the standard tiling approach is:\n\nEach thread block computes a tile of C of size BLOCK_SIZE x BLOCK_SIZE.\n\nEach thread in the block computes one element of this tile.\n\nTo compute C[row][col], the element is the sum over k of A[row][k] * B[k][col].\n\nBut since we are tiling the K dimension, we divide K into blocks of size TILE_SIZE (same as BLOCK_SIZE). For each tile k, we load a BLOCK_SIZE x TILE_SIZE slice of A and a TILE_SIZE x BLOCK_SIZE slice of B into shared memory. Then, each thread can compute its part.\n\nWait, perhaps the shared memory for A should be of size BLOCK_SIZE (rows) x TILE_SIZE (columns), and for B TILE_SIZE (rows) x BLOCK_SIZE (columns). But since TILE_SIZE is equal to BLOCK_SIZE, we can use BLOCK_SIZE for both.\n\nAlternatively, using the blocking method where each thread block processes a BLOCK_SIZE x BLOCK_SIZE tile of C, and for each tile of K (size BLOCK_SIZE), we load the corresponding BLOCK_SIZE x BLOCK_SIZE tiles of A and B into shared memory.\n\nTherefore, the shared_A is a BLOCK_SIZE (rows) x BLOCK_SIZE (columns) matrix, but actually, for each k tile, the columns of A would be from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1. So each thread in the block (threadIdx.y, threadIdx.x) will load A\'s element at (row, k*BLOCK_SIZE + threadIdx.x) into shared_A[threadIdx.y][threadIdx.x].\n\nSimilarly, for B, the rows are k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and the columns are col. So each thread (threadIdx.y, threadIdx.x) loads B\'s element at (k*BLOCK_SIZE + threadIdx.y, col) into shared_B[threadIdx.y][threadIdx.x].\n\nThen, the product of shared_A and shared_B is a BLOCK_SIZE x BLOCK_SIZE matrix, where each element (i,j) is sum over m of shared_A[i][m] * shared_B[m][j]. Wait, no. Actually, the multiplication of the two shared matrices (A_tile and B_tile) would be:\n\nC_tile += A_tile (BLOCK_SIZE x BLOCK_SIZE) * B_tile (BLOCK_SIZE x BLOCK_SIZE)?\n\nNo, actually, the B matrix is KxN, so when we take a tile of B of size BLOCK_SIZE x BLOCK_SIZE (rows k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and columns are the block\'s column tile), then the multiplication of A\'s tile (M x BLOCK_SIZE) and B\'s tile (BLOCK_SIZE x N) would give a BLOCK_SIZE x BLOCK_SIZE tile for C.\n\nWait, perhaps the dimensions need to be:\n\nA\'s tile for the current k is of size BLOCK_SIZE (rows) x BLOCK_SIZE (columns), since the columns are from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1.\n\nB\'s tile is BLOCK_SIZE (rows) x BLOCK_SIZE (columns), since the rows are from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and the columns are the current block\'s columns.\n\nTherefore, the multiplication of the two shared matrices (A_tile [BLOCK_SIZE x BLOCK_SIZE] and B_tile [BLOCK_SIZE x BLOCK_SIZE]) would result in a BLOCK_SIZE x BLOCK_SIZE matrix, which is added to the C tile.\n\nWait, actually, the matrix multiplication of A\'s tile (size BLOCK_SIZE x BLOCK_SIZE) and B\'s tile (size BLOCK_SIZE x BLOCK_SIZE) would be a BLOCK_SIZE x BLOCK_SIZE matrix, which contributes to the current C tile.\n\nTherefore, the kernel code should be structured as follows:\n\nEach thread in the block is responsible for one element in the C tile. The thread\'s position is (threadIdx.y, threadIdx.x), corresponding to row and column in the tile.\n\nFor each k tile:\n\n1. Load the A tile: A\'s rows are the current block\'s rows (row = blockIdx.y * BLOCK_SIZE + threadIdx.y), and columns are from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1. So each thread loads A[row][k*BLOCK_SIZE + threadIdx.x] into shared_A[threadIdx.y][threadIdx.x].\n\n2. Load the B tile: B\'s rows are from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and columns are the current block\'s columns (col = blockIdx.x * BLOCK_SIZE + threadIdx.x). So each thread loads B[k*BLOCK_SIZE + threadIdx.y][col] into shared_B[threadIdx.y][threadIdx.x].\n\nWait, the B matrix is stored in row-major order, so B\'s element at (row_B, col_B) is B[row_B * N + col_B]. Therefore, for the B tile, the row in B is k*BLOCK_SIZE + threadIdx.y, and the column is col (which is blockIdx.x * BLOCK_SIZE + threadIdx.x). So the index is (k*BLOCK_SIZE + threadIdx.y) * N + col.\n\nThen, after loading both shared_A and shared_B, we perform the multiplication for the current tile:\n\nFor each element in the C tile, the thread computes the sum over the shared_A and shared_B:\n\nfor (int m = 0; m < BLOCK_SIZE; ++m) {\n    C_value += shared_A[threadIdx.y][m] * shared_B[m][threadIdx.x];\n}\n\nWait, yes. Because shared_A is [threadIdx.y][m] (row, column), and shared_B is [m][threadIdx.x] (row, column). So the dot product over m gives the element at (threadIdx.y, threadIdx.x) in the C tile.\n\nTherefore, the kernel code should look like this:\n\n```cuda\n#define BLOCK_SIZE 16\n\n__global__ void matrix_multiply_kernel(const float* A, const float* B, float* C, \n                                      int M, int N, int K) {\n    __shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float shared_B[BLOCK_SIZE][BLOCK_SIZE];\n\n    // Compute the row and column indices in the output matrix C\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n\n    float C_value = 0.0;\n\n    // Loop over all the tiles of K\n    for (int k = 0; k < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; ++k) {\n        // Calculate the index in A and B for this tile\n        int a_col = k * BLOCK_SIZE + threadIdx.x;\n        int b_row = k * BLOCK_SIZE + threadIdx.y;\n\n        // Load the current tile of A into shared memory\n        if (a_col < K) {\n            shared_A[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            shared_A[threadIdx.y][threadIdx.x] = 0.0;\n        }\n\n        // Load the current tile of B into shared memory\n        if (b_row < K && col < N) {\n            shared_B[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            shared_B[threadIdx.y][threadIdx.x] = 0.0;\n        }\n\n        __syncthreads();\n\n        // Compute the product of the shared tiles and accumulate\n        for (int m = 0; m < BLOCK_SIZE; ++m) {\n            C_value += shared_A[threadIdx.y][m] * shared_B[m][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    // Write the result back to global memory\n    if (row < M && col < N) {\n        C[row * N + col] = C_value;\n    }\n}\n```\n\nWait, there\'s a potential issue here. The shared_B is loaded as B\'s rows (b_row) and columns (col). But in the kernel, the column for B is fixed as col (the column in C), so each thread in the block is responsible for loading one element of B\'s tile. The shared_B is organized such that shared_B[threadIdx.y][threadIdx.x] corresponds to B\'s element at (b_row, col). However, since the B tile\'s columns are fixed to the current block\'s column tile, the column in B is actually the same for all threads in the block. Wait, no. The column in B is col, which is the same for all threads in the block. Therefore, each thread in the block is responsible for loading a row of the B tile\'s column.\n\nWait, no. Let me clarify:\n\nThe B matrix is of size KxN. The current block is responsible for the C tile at (blockIdx.y, blockIdx.x), which corresponds to rows [blockIdx.y*BLOCK_SIZE, ..., ) and columns [blockIdx.x*BLOCK_SIZE, ...].\n\nThe B tile for the current k iteration is the subset of B\'s rows from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and columns exactly the current block\'s column tile, which is col = blockIdx.x*BLOCK_SIZE + threadIdx.x. Wait, no. The column in B is the same as the column in C, which is col. But since the block is processing a BLOCK_SIZE x BLOCK_SIZE tile in C, the columns in C for this block are from blockIdx.x*BLOCK_SIZE to (blockIdx.x+1)*BLOCK_SIZE -1. Therefore, each thread in the block is responsible for a specific column in this range. Therefore, for the B matrix, the column is col (the same as in C), and the row is varying over the current k tile.\n\nTherefore, for the B tile, each thread in the block (threadIdx.y, threadIdx.x) is responsible for loading the element B[b_row][col], where b_row = k*BLOCK_SIZE + threadIdx.y. However, the column col is fixed for the entire block. Wait, no. Each thread in the block is responsible for a different column in the C tile. The col variable for the C element is blockIdx.x*BLOCK_SIZE + threadIdx.x. Therefore, for the B matrix, the column is exactly that col. Therefore, each thread in the block needs to load B\'s element at (b_row, col). However, since all threads in the block have the same col (because col is computed as blockIdx.x*BLOCK_SIZE + threadIdx.x, which varies per thread), this approach won\'t work because each thread\'s col is different.\n\nAh, here\'s the mistake. The previous approach assumes that all threads in the block are working on the same column, which is not the case. Each thread in the block is responsible for a different column in the C tile. Therefore, the B matrix\'s column for each thread is different.\n\nThis means that the way we load B into shared memory is incorrect. The shared_B should be a tile of B of size BLOCK_SIZE (rows) x BLOCK_SIZE (columns), where the rows are the current k tile, and the columns are the current block\'s column tile.\n\nWait, that\'s the correct approach. The B tile should be a BLOCK_SIZE x BLOCK_SIZE matrix, where the rows are from k*BLOCK_SIZE to (k+1)*BLOCK_SIZE -1, and the columns are from blockIdx.x*BLOCK_SIZE to (blockIdx.x+1)*BLOCK_SIZE -1. Therefore, each thread in the block needs to load one element of this
"""

model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
generated_ids = model.generate(**model_inputs, temperature=1.2, max_new_tokens=128)
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print(response)